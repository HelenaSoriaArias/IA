# Inteligencia artificial: Un enfoque moderno

## Portada
Ensayo
* Alumno : Soria Arias Nora Helena
* Materia: Inteligencia Artificial
* Maestro: Alcaráz Chavéz Jesús Eduardo
* Fecha  : 07 de Diciembre del 2023

## Introducción
El presente ensayo es una análisis y opinión personal sobre los capítulos 1, 2, 26, y 27 del libro _Inteligencia Artificial: Un enfoque moderno_ de Russell. Con el fin de respetar el material original el análisis será desarrollado en el mismo orden que en el libro. Los temas a tratar son _Introducción_, _Agentes Inteligentes_, _Fundamentos filosofícos_ e _IA: presente y futuro_.

Abreviaciones: **IA** = Inteligencia Artificial

## Inteligencia Artificial : Un enfoque moderno

### Capítulo 1

El primer capítulo _Introducción_ está escrito como preludio a todos los siguientes temas y esta escrito de manera ligera y fácil de entender para novatos en el tema de la inteligencia artificial y Russel comienza con la pregunta más evidente: **¿Qué es la Inteligencia Artificial**. Y propone 4 respuestas posibles, considerando las declaraciones de anteriores autores y los conceptos de _racionalidad_ e _humanidad_. Mientras que la racionalidad hace referencia a la inteligencia y a la toma de decisiones correctas, la humanidad habla sobre la capaidad de las máquinas de pretender ser humano o de emular los comportamientos y acciones humanas con la mayor fidelidad posible. 

Los 4 enfoques que muestra Russel son:
 1. Sistemas que piensan como humanos
**Enfoque del sistema cognitivo**
Este enfoque es poco preciso, ya que explica que si una máquina quiere pensar como un humano, primero tenemos que determinar cómo piensan los humanos. No hay una respuesta definitiva para como conseguir la imitación  del pensamiento humano, pero señala a la _ciencica cogninitva_ cómo la respuesta para este dilema.

 2. Sistemas que actuan como humanos
 **Enfoque de la prueba de Turing**
 Para esta definición de IA, se dice que una IA es aquella máquina que actua como humanos. Y el sistema de evaluación de la máquina es la Prueba de Turing, que consiste en convencer a una persona (por medio de una corta conevrsación) que la IA en realidad es un humano. En este caso la máquina debe de contar con 6 cualidades para siquiera ser considerada para la prueba.
• Procesamiento de lenguaje natural.
• Representación del conocimiento.
• Razonamiento automático.
• Aprendizaje automático 
•Visión computacional para percibir objetos.
• Robótica para manipular y mover objetos.

 3. Sistemas que piensan racionalmente
 **Enfoque de las leyes del pensamiento**
 Este enfoque se puede explicar por medio de las leyes del pensamiento racional escritas por Aristóteles, quién definió a los silogismos como  leyes del pensamiento o estructuras de argumentación que conducen a conclusiones correctas. Esto se entiende de manera muy sencilla: una manera formal de ganar argumentos o demostrar la validez de ciertas ideas. En contraste, la lógica es la ciencia que estudia los silogismos y contiene la notación necesaria para explicarlos, similar a la notación matemática y es generalmente muy útil para encontrar la verdad a partir de deducciones. Sin embargo, la lógica tiene no es perfecta y presenta ciertas debilidades: en primera grandes cantidades de conocimiento son díficiles de procesar, y la segunda es que los computadores actuales no tienen la suficiente capacidad de procesamiento para resolver los problemas lógicos.
 
 4. Sistemas que actuan racionalmente 
 **Enfoque del agente racional**

En cuanto a las disciplinas que fundamnetan la inteligencia arificial tenemos a la filosofía, las matemáticas, la economía, la neurociencia, la psicología, las ciencias computacionales , la cibernética y la linguistica. Sin embargo, no es necesario hablar de todas ellas, se pueden acotar con las filosofía, las matematicas y las ciencias computacionales.

1. Filosofía

Una de las ciencias con más impacto en las raíces de la IA, en la actualidad puede que no sea tan evidente su influencia cómo las matemáticas y las ciencias computacionales, pero en sus origenes es la más relevante. Iniciando con los silogismos ya mencionados de Aristoteles, hay una serie de investigadores que merecen una mención.

**Ramón Lull (1315)** Plantea la idea de hacer razonamiento artificial.

**Thomas Hobbes (1588-1679)** Propone aplicar operaciones aritmeticas al razonamiento.

**Leonardo da Vinci (1452-1519)** Diseña una calculadora mecánica.

**Wilhelm Schickard (1592-1635)** Crea la primera caluladoradora.

**Blaise Pascal (1623-1662)** Crea la caluladora Pascalina.

**Wilhelm Leibniz (1646-1716)** Crea una calculadora sobre conceptos y no números.

**René Descartes (1596-1650)** Define la distinción entre la mente y la materia.
Aquí es importante agregar una distinción entre 2 conceptos que dominaron la época de Descartes: materialismo vs dualismo. El materialismo indica que la mente se expresa por medio del cerebro y todos los procesos internos que ocurren adentro de el, mientras que el dualismo indica que la mente existe afuera del plano terrenal.

**Francis Bacon  (1561-1626)** Inicia el movimiento empirico.
En el movimiento empirico las cosas existen porque son perceptibles por los sentidos.

**David Humme  (1711-1776)** Propone el principio de Inducción.
El principio de Inducción dice que todas las reglas se pueden inducir después de múltiples exposiciones a la la misma regla.

**Ludwig Wittgenstein (1889-1951) y Bertrand Russell (1872-1970)** Desarrollan el positivismo lógico.
Definen el positivismo lógico como una serie de sentencias lógicas procedentes de la observación.

**Carnap y Carl Hempel (1905-1997)** Definen la teoría de la confirmación
Esta teoría indica que todo el conocimiento sale de la experiencia.

2. Matemáticas

Los investigadores que resaltan en esta rama con sus contribuciones a la IA en este campo son:

**George Boole (1815-1864)**    Crea la lógica booleana.

**Gottlob Frege (1848-1925)**   Extiende la lógica booleana a lógica de primer orden.

**Alfred Tarski (1902-1983)**   Diseña las relaciones concepto-objeto.

**David Hilbert (1862-1943)**   Presenta 23 problemas para los investigadores del siglo, uno de ellos habla de los algoritmos para proposiciones lógicas con números naturales

**Kurt Gödel (1906-1978)**      Afirma que las proposiciones lógicas no pueden resolverse si incluyen números naturales y desarrolla el teorema de la incompletud.  

**Alan Turing (1912-1954)** Desarrolla la máquina de Turing.

Alrededor de esta época surge el concepto **Intratabilidad**, el cuál es vigente incluso en la actualidad y explica que si la díficultad de un problema crece exponencialmente es imposible de resolver y se recomienda quebrarlo en partes para solucionarlos poco a poco.

**Steven Cook (1971) y Richard Karp (1972)** Desarrollan la teoría de la **NP-Completitud** donde explican los problemas de combinatorias intratables.

**Gerolamo Cardano (1501-1576)** Desarrolla la teoría de la probabilidad.

**Thomas Bayes (1702-1761)** Aporta a la probabilidad la teoría de la _Regla de Bayes_ y el _Análisis Bayesiano_ sobre la probabilidad subjetiva.


3. Ciencias computacionales
La inteligencia artificial necesita un artefacto para poder manifestarse en el mundo real; y la computación ha resultado ser el artefacto más conveniente para la misma. De las grandes figuras en este campo se encuentra:

**Joseph Marie Jacquard (1752-1834)** Desarrolla un telar programable con tarjetas perforadas.

**Charles Babbage (1792-1871)** Diseña máquinas calculadoras.

**Ada Lovelace** Primera programadora de la historia.

**Alan Turing (1912-1954)** Desarrolla la primera computadora durante la Segunda Guerra Mundial.

**Konrad Zuse** Desarrolla la computadora Z-3.

**John Atanasoff y Clifford Berry** Desarrollan la computadora ABC.

**John Mauchly y John Eckert** Desarrollan la computadora ENIAC

#### Sobre la inteligencia artificial
Habiendo ya definido IA y mencionado a las disciplinas que ayudaron al desarrollo de la IA adempas de incluir a las personas que de alguna manera tuvieron influerncia sobre el avance de la misma, es momento de entrar a fondo en la historia de la IA. El terminó ***Iinteligencia Artificial*** fu acunado por primera vez en 1956 en el verano de investigación de John McCarthy, Marvin Minsky, Nathaniel Rochester y Claude Shannon. Gracias a ese verano de investigación, McCarthy, colegas y alumnos dominaron el campo durante las siguientes décadas.

El siguiente avance relevante en la época, fue hecho por Allen Newell y Herbert Simon, ambos alumnos de McCarthy al desarrollar un SGRP o sistema de resolución general de problemas por medio de protocolos que simulaban el pensamiento humano, siendo de los primeros que usan el _enfoque del sistema cognitivo_. También desarrollan la _Hipótesis de símbolos físicos_ donde declaran que todo sistema debe de tener un conjunto de símbolos para poder ser operado.

MaCarthy no se quedó atrás, siguió con sus investigaciones y en 1958 tuvo grandes avances: desarrollo el lenguaje de programación LISP que se basa en reglas y hechos para lograr un comportamiento, después creó el uso compartido de recursos computacionales y por último propuso la idea de un programa con sentido común. Por otro lado, Minsky se dedico al desarrollo de sofware y a la resolución de problemas en entornos controlados, a los cuales bautizaron "_micromundos_", los cuales resolvian problemas de cálculo que tuvieran una estructura muy específica.

El tema se vuelve interesante cuando se acaban los exitos y se emepizan a ver las límitaciones reales de la inteligencia artificial, hasta el momento habían ocurrido muchos exitos, pero sólo ocurrieron porque se la IA se utilizaba en espacios controlados con variables controladas, en el momento en que la IA tuvo que salir al mundo y enfrentar problemas complejos de la vida real falló por completo. Además, el hardware de computo no se encontraba lo suficientemente desarrollado como para obtener avances significativos.  Cómo resultado de todo esto la IA tuvo una gran perdida de financiamientos y por lo tanto las investigaciones y el avance fue frenado.

Considero que para los investigadores de la época debe haber sido un duro golpe saber que sus descubrimientos e investigaciones no estaban siendo reconocidas y de que deberían buscar otra fuente de sustento para su vida, sin embargo creo que dejar en pausa la inteligenica artificial por un tiempo también tuvo sus ventajas, pues permitió que el mercado del hardware y software se pusiera a la par con lo que requería la IA.

En contraste, en la actualidad la IA esta tomando cada día más fuerza y se puede encontrar en diversas areas del conocimiento, por un lado tenemos a la NASA que se encarga de monitorear las operaciones de las naves espaciales, por otro lado la industria de los videojuegos donde la IA simplifica el desarrollo o incluso la IA se vuelve jugador.


### Capítulo 2

_Agentes inteligentes_ es el capítulo 2 del libro y se enfoca en los agentes racionales y la capacidad de tomar la mejor decisión de acuerdo a la circunstancia. En orden de explicar esto, lo primero es definir **agente** como cualquier cosa que recibe señales del exterior con **sensores** e interactua con el mundo real con **actuadores**, aunque el libro también provee una definición más formal para el concepto de **agente racional**, un tipo específico de agentes en los cuales profundiza más este capítulo.  

**Definición de agente racional**
>En cada posible secuencia de percepciones, un agente racional deberá emprender aquella acción que supuestamente maximice su medida de rendimiento, basándose en las evidencias aportadas por la secuencia de percepciones y en el conocimiento que el agente
mantiene almacenado.

Lo sorprendente es que los humanos también son agentes racionales, utilizamos la **percepción** (entradas de información a nuestro cerebro), mientras que nuestra memoria seria el equivalente a una **secuencia de percepciones**. 

**Función de agente**
El comportamiento teorico matemático que determina la decisión del agente en comparación con cierta secuencia de percepciones,se puede representar en 1 tabla de un lado mostrando la secuencia y del otro el resultado.

**Programa del agente**
Es la implementación en código de la función del agente.

**Arquictura del agente**
Es la implementación física de un agente, los componentes electronicos que físicamente forman al agente y permiten que corra su programa.

**Medidas de rendimiento**
Factores que sirven como metrica para determinar el exito, fracaso y rendimiento de un agente racional.

**Omniciencia**
La omniciencia de un agente es que este sepa el futuro que ocurrira con las decisiones que puede tomar, sin embargo esto es imposible. El agente solo sabe que ocurrira en base a sus entradas, y si los usuarios no le agregan entradas no calculara efectivamente el futuro. Y los propios usuarios no conocen el futuro, entonces no agregan la información correspondiente y por lo tanto el agente tiene una visión del futuro limitada. Se concluye que a menos que las bolas de cristal o los viajeros en el tiempo den información correcta no existiran agentes perfectos.

**Entornos de trabajo (REAS)**
Un entorno de trabajo es todas las espeçificaciones y variables relevantes para resolver un problema por mediod e un agente. La cantidad y tipo de sensores, actuadores, las medidas de rendimiento y el contexto donde se va a utilizar el agente son todas partes de un entorno de trabajo. Entre sus características hay:

    1. Visible o parcialmente visible.
    
    Si los actuadores detectan todo el entorno en todo momento, es visible. Si hay limitaciones de tiempo o alcance es parcialmente visible.

    2.Determinista vs. estocástico.

    La cantidad de influencia del estado actual sobre el agente define si es determinista o estocástico. Determinística si hay mucha influencia, estocástico si el entorno no afecta al agente.

    3.

### Capítulo 26
El capítulo 26: _Fundamentos filosóficos_ es de los más interesantes para examinar porque invita a la reflexión del lector con multiples preguntas, entre ellas la más importante es : _¿Las máquinas realmente piensan?_. Con esto abre 2 hipótesis: la hipótesis de la **IA fuerte** y la hipótesis de la **IA débil**.La IA débil asume que las máquinas con IA no tienen verdadera inteligencia, ya que sólo hacen lo que se les programo para hacer y en realidad no aportan nada propio. En contraste, la hipótesis de la IA fuerte hace referencia a máquinas con la misma capaidd de pensar de los humanos.

Desde mi punto de vista, la mayor debílidad de la IA débil es que sus limitaciones yacen el el ser humano que la maneje, tanto la capacidad de procesamiento como sus acciones están límitadas por lo que los humanos definan para ella, entonces nunca llegará a expresar creatividad e individualidad como un ser humano normal. Por otro lado Turing propuso que no es necesario enforcarse tanto en sí las máquinas piensan o no porque la respuesta a esa pregunta no lleva a resultados prácticos o aplicables; mientras la IA cumpla su próposito y haga su trabajo poco importa su inteligencia.

Siguiendo este enfoque, Turing crea el Test de Turing, una prueba cognitiva para las máquinas. Consiste en poner a la máquina a tener una conversación de 5 minutos con un ser humano y convencer a dicho humano de que se encuentra teniendo una conversación con otro ser humano. En pocas palabras, pretender ser humano por 5 minutos y si el porcentaje de exito es el 30% entonces pasó el test con éxito; sin embargo engañar a personas despistadas por medio de una conversación de 5 minutos no es una medida realista para definir la inteligencia de una máquina, y por lo tanto este Test no debería de usarse para determinar la diferencia entre IA débil e IA fuerte.

Turing también define 3 conceptos importantes: el _argumento de la incapacidad_, el _argumento de la informalidad_ y la _objeción matemática_. El primero hace referencia a todas las cosas imposibles para una máquina, como tener sentimientos, enamorarse o disfrutar de la comida; este argumento es muy obvio, pero antes de Turing ningún investigador había planteado formalmente todas las cosas que una máquina nopodía hacer.

En el otro extremo está el _argumento de la informalidad_ dónde explica que el comportamiento de las personas es demasiado complejo, implica demasiadas variables y en ocasiones es tan impredecible como para que lo puedan poder reducri a un conjunto de reglas y utilizar esas reglas en una máquina.Hubert Dreyfus complementa este tema al analizar las **GOFAI** o **Good old fashioned AI**; las GOFAI se caracterizan porque no se salen de un conjunto de reglas y hechos, nunca tienen comportamientos inesperados.


>Para especificar, Dreyfuis no creó el termino, el sólo escribió sobre el y analizó el tema, quién realmente lo escribio fue Haugeland (1985)

### Capítulo 27

Este es el último capítulo del libro y habla, como su título sugiere, del presente y futuro que nos espera en la IA. Antes de continuar quiero aclarar que este libro fue publicado en 2004, por lo que los sistemas que dicen ser presente no están actualizados a la fecha (2023).
Se menciona como la entrada de datos a un agente ha sido uno de los mayores problemas de la Inteligencia Artificial,en contraste puedo decir que la entrada de datos se ha facilitado desde el punto de vista técnico, pero no desde el punto de vista financiero. El desarrollo de agentes racionales y la robótica sigue considerandose una actividad cara, aunque no inalcanzable.

También se habla de las arquiecturas de los agentes, no detallandolas como antes, sino específicando las ventajas y desventajas de cada una, de manera que el lector sea quién tome la decisión de cual de todas es la que más necesita, y tomar en cuenta que el objetivo de estos agentes es que tomen decisiones en tiempo real, por lo que hay un límite de lo que se puede controlar.

Lo más interesante de este capítulo es la pregunta de adonde nos llevará la IA, y si nos encontramos en la busqueda de la *racionalidad perfecta*, que significa tomar la mejor decisión, en el mejor momento, en las mejores condiciones. ¿Qué ocurre cuando esto no se cumple? Si ya es muy tarde para tomar la mejor decisión, sólo se tiene *racionalidad calculada*; si no hay las mejores condiciones, entonces es *racionalidad limitada* y si la mejor opción no esta disponible por cualquier razón, entonces es *optimalidad limitada*.

Con los conceptos anteriores sabemos que la IA sigue el camino de la optimalidad limitada, los humanos no tienen idea de cual es la mejor y perfecta IA, no pueden crearla en el instante y existen las limitaciones de recursos (financieros y tecnológicos) para poder llegar a la IA perfectamente racional.

Hacia el final se indaga en la cuestión ética, incluso si en el futuro se desarrolla la IA perfectamente racional y se hace un salto tecnológico exponencial, queda la duda de si no podrá ser utilizado con propósitos cuestionables.Considerando que los computadores se han vuelto cada vez más importantes en la sociedad y que la IA esta lentamente abarcando aspectos de la vida cotidiana, el impacto a la sociedad sería mayor de ser utilizado para mal.

También existe el impacto en el trabajo y la economía, porque aquellos con aptitudes para la tecnológia tienen más oportunidades laborales y salarios más altos que otros, al menos según los autores. La realidad del 2023 es que las empresas ya no contratan personas que no sepan manejar ordenadores, no todos los trabajos tienen que ver con las computación, pero se suele pedir un mínimo de herramientas electronicas.

En cuanto a un futuro apocalíptico como los que sugieren los autores de ciencia ficción, deben quedarse adentro de la ciencia ficción y no aparecer en el mundo real, porque los futuros post-apocaliptiicos donde gobiernan las máquinas no le convienen a la sociedad humana.

## Conclusiones

Mi opinión con respecto al capítulo 1 es que es altamente historico,ese capítulo por si sólo tiene material para extenderse hasta todo un libro nada más hablando de las figuras involucradas en el desarrollo de la IA. A ratos me pareció interesante  y a ratos irrelevante, dependiendo de la figura historico a la que se estuviera refiriendo; de cualquier manera, como introducción no es un mal contenido. El capítulo 2 es, al contrario, lleno de tecnicismos y específicaciones sobre los agentes inteligentes.

En contraste, los ultimos capítulos fueron muy interesantes por que hablan del lado filosofico y ético de la IA, tanto como desarrollador como miembro de la sociedad  es importante pensar que uso y que implicaciones tiene hacer que las máquinas piensen.

## Referencias

RUSSELL, S. J.; NORVIG, P. INTELIGENCIA ARTIFICIAL. UN ENFOQUE MODERNOSegunda edición
PEARSON EDUCACIÓN, S.A., Madrid, 2004. ISBN: 978-84-205-4003-0
 